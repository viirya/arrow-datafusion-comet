== Physical Plan ==
* ColumnarToRow (26)
+- CometHashAggregate (25)
   +- CometColumnarExchange (24)
      +- CometHashAggregate (23)
         +- CometProject (22)
            +- CometSortMergeJoin (21)
               :- CometSort (10)
               :  +- CometHashAggregate (9)
               :     +- CometColumnarExchange (8)
               :        +- RowToColumnar (7)
               :           +- * HashAggregate (6)
               :              +- * Project (5)
               :                 +- * BroadcastHashJoin Inner BuildRight (4)
               :                    :- * ColumnarToRow (2)
               :                    :  +- CometScan parquet spark_catalog.default.store_sales (1)
               :                    +- ReusedExchange (3)
               +- CometSort (20)
                  +- CometHashAggregate (19)
                     +- CometColumnarExchange (18)
                        +- RowToColumnar (17)
                           +- * HashAggregate (16)
                              +- * Project (15)
                                 +- * BroadcastHashJoin Inner BuildRight (14)
                                    :- * ColumnarToRow (12)
                                    :  +- CometScan parquet spark_catalog.default.catalog_sales (11)
                                    +- ReusedExchange (13)


(unknown) Scan parquet spark_catalog.default.store_sales
Output [3]: [ss_item_sk#1, ss_customer_sk#2, ss_sold_date_sk#3]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#3), dynamicpruningexpression(ss_sold_date_sk#3 IN dynamicpruning#4)]
ReadSchema: struct<ss_item_sk:int,ss_customer_sk:int>

(2) ColumnarToRow [codegen id : 2]
Input [3]: [ss_item_sk#1, ss_customer_sk#2, ss_sold_date_sk#3]

(3) ReusedExchange [Reuses operator id: 31]
Output [1]: [d_date_sk#5]

(4) BroadcastHashJoin [codegen id : 2]
Left keys [1]: [ss_sold_date_sk#3]
Right keys [1]: [d_date_sk#5]
Join type: Inner
Join condition: None

(5) Project [codegen id : 2]
Output [2]: [ss_item_sk#1, ss_customer_sk#2]
Input [4]: [ss_item_sk#1, ss_customer_sk#2, ss_sold_date_sk#3, d_date_sk#5]

(6) HashAggregate [codegen id : 2]
Input [2]: [ss_item_sk#1, ss_customer_sk#2]
Keys [2]: [ss_customer_sk#2, ss_item_sk#1]
Functions: []
Aggregate Attributes: []
Results [2]: [ss_customer_sk#2, ss_item_sk#1]

(7) RowToColumnar
Input [2]: [ss_customer_sk#2, ss_item_sk#1]

(8) CometColumnarExchange
Input [2]: [ss_customer_sk#2, ss_item_sk#1]
Arguments: hashpartitioning(ss_customer_sk#2, ss_item_sk#1, 5), ENSURE_REQUIREMENTS, CometColumnarShuffle, [plan_id=1]

(9) CometHashAggregate
Input [2]: [ss_customer_sk#2, ss_item_sk#1]
Keys [2]: [ss_customer_sk#2, ss_item_sk#1]
Functions: []

(10) CometSort
Input [2]: [customer_sk#6, item_sk#7]
Arguments: [customer_sk#6, item_sk#7], [customer_sk#6 ASC NULLS FIRST, item_sk#7 ASC NULLS FIRST]

(unknown) Scan parquet spark_catalog.default.catalog_sales
Output [3]: [cs_bill_customer_sk#8, cs_item_sk#9, cs_sold_date_sk#10]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#10), dynamicpruningexpression(cs_sold_date_sk#10 IN dynamicpruning#11)]
ReadSchema: struct<cs_bill_customer_sk:int,cs_item_sk:int>

(12) ColumnarToRow [codegen id : 4]
Input [3]: [cs_bill_customer_sk#8, cs_item_sk#9, cs_sold_date_sk#10]

(13) ReusedExchange [Reuses operator id: 31]
Output [1]: [d_date_sk#12]

(14) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [cs_sold_date_sk#10]
Right keys [1]: [d_date_sk#12]
Join type: Inner
Join condition: None

(15) Project [codegen id : 4]
Output [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Input [4]: [cs_bill_customer_sk#8, cs_item_sk#9, cs_sold_date_sk#10, d_date_sk#12]

(16) HashAggregate [codegen id : 4]
Input [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Keys [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Functions: []
Aggregate Attributes: []
Results [2]: [cs_bill_customer_sk#8, cs_item_sk#9]

(17) RowToColumnar
Input [2]: [cs_bill_customer_sk#8, cs_item_sk#9]

(18) CometColumnarExchange
Input [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Arguments: hashpartitioning(cs_bill_customer_sk#8, cs_item_sk#9, 5), ENSURE_REQUIREMENTS, CometColumnarShuffle, [plan_id=2]

(19) CometHashAggregate
Input [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Keys [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Functions: []

(20) CometSort
Input [2]: [customer_sk#13, item_sk#14]
Arguments: [customer_sk#13, item_sk#14], [customer_sk#13 ASC NULLS FIRST, item_sk#14 ASC NULLS FIRST]

(21) CometSortMergeJoin
Left output [2]: [customer_sk#6, item_sk#7]
Right output [2]: [customer_sk#13, item_sk#14]
Arguments: [customer_sk#6, item_sk#7], [customer_sk#13, item_sk#14], FullOuter

(22) CometProject
Input [4]: [customer_sk#6, item_sk#7, customer_sk#13, item_sk#14]
Arguments: [customer_sk#6, customer_sk#13], [customer_sk#6, customer_sk#13]

(23) CometHashAggregate
Input [2]: [customer_sk#6, customer_sk#13]
Keys: []
Functions [3]: [partial_sum(CASE WHEN (isnotnull(customer_sk#6) AND isnull(customer_sk#13)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (isnull(customer_sk#6) AND isnotnull(customer_sk#13)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (isnotnull(customer_sk#6) AND isnotnull(customer_sk#13)) THEN 1 ELSE 0 END)]

(24) CometColumnarExchange
Input [3]: [sum#15, sum#16, sum#17]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, CometColumnarShuffle, [plan_id=3]

(25) CometHashAggregate
Input [3]: [sum#15, sum#16, sum#17]
Keys: []
Functions [3]: [sum(CASE WHEN (isnotnull(customer_sk#6) AND isnull(customer_sk#13)) THEN 1 ELSE 0 END), sum(CASE WHEN (isnull(customer_sk#6) AND isnotnull(customer_sk#13)) THEN 1 ELSE 0 END), sum(CASE WHEN (isnotnull(customer_sk#6) AND isnotnull(customer_sk#13)) THEN 1 ELSE 0 END)]

(26) ColumnarToRow [codegen id : 5]
Input [3]: [store_only#18, catalog_only#19, store_and_catalog#20]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = ss_sold_date_sk#3 IN dynamicpruning#4
BroadcastExchange (31)
+- * ColumnarToRow (30)
   +- CometProject (29)
      +- CometFilter (28)
         +- CometScan parquet spark_catalog.default.date_dim (27)


(unknown) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#5, d_month_seq#21]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_month_seq:int>

(28) CometFilter
Input [2]: [d_date_sk#5, d_month_seq#21]
Condition : (((isnotnull(d_month_seq#21) AND (d_month_seq#21 >= 1200)) AND (d_month_seq#21 <= 1211)) AND isnotnull(d_date_sk#5))

(29) CometProject
Input [2]: [d_date_sk#5, d_month_seq#21]
Arguments: [d_date_sk#5], [d_date_sk#5]

(30) ColumnarToRow [codegen id : 1]
Input [1]: [d_date_sk#5]

(31) BroadcastExchange
Input [1]: [d_date_sk#5]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=4]

Subquery:2 Hosting operator id = 11 Hosting Expression = cs_sold_date_sk#10 IN dynamicpruning#4


